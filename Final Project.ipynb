{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhySHcOJfQCx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3diVO6f1fQC0",
        "outputId": "34f2cf55-557b-47d7-f417-153244dfc70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=f8a08bf70e54acbc858910107ab9846bd27a7df2e89975880048c7527b21243d\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from memory_profiler import profile\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvOXYBeYfQC1"
      },
      "source": [
        "## Defining global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utn19DaKfQC1"
      },
      "outputs": [],
      "source": [
        "change_ratio = 0.9\n",
        "sampling_ratio = 0.75\n",
        "n_clusters = 5\n",
        "ratio = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "N = int(22544 * ratio[0])   # N is the number of initial data to be clustered.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJIH8xdhfQC2"
      },
      "source": [
        "## Defining utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_asdE9YfQC3"
      },
      "outputs": [],
      "source": [
        "# Function to create list of lists.\n",
        "\n",
        "def init_list_of_objects(size):\n",
        "    list_of_objects = list()\n",
        "    for i in range(0,size):\n",
        "        list_of_objects.append(list())\n",
        "    return list_of_objects\n",
        "\n",
        "# Function to create a list of dictionnaries.\n",
        "\n",
        "def init_list_of_dict(size):\n",
        "    list_of_objects = list()\n",
        "    for i in range(0,size):\n",
        "        list_of_objects.append(dict())\n",
        "    return list_of_objects\n",
        "\n",
        "# Defining our sampling probability. \n",
        "\n",
        "def probability(e, center):\n",
        "    norm_e = np.linalg.norm(e)\n",
        "    norm_center = np.linalg.norm(center)\n",
        "    return int(abs(1 - norm_e / norm_center) * 100)\n",
        "\n",
        "# Defining our error function.\n",
        "\n",
        "def calculateErrorWithSample(e, sample, init_mean):\n",
        "    sample = sample.append(e, ignore_index = True)\n",
        "    sample_mean = calculateMean(sample)\n",
        "    overall_mean = init_mean - sample_mean\n",
        "    result = overall_mean.div(init_mean).replace(np.inf, 0)\n",
        "    return  abs(result.mean()) * 100  \n",
        "\n",
        "def calculateErrorWithoutSample(sample, init_mean):\n",
        "    sample_mean = calculateMean(sample)\n",
        "    overall_mean = init_mean - sample_mean\n",
        "    result = overall_mean.div(init_mean).replace(np.inf, 0)\n",
        "    return  abs(result.mean()) * 100  \n",
        "\n",
        "# calculating the mean for a giving cluster.\n",
        "\n",
        "def calculateMean(df):\n",
        "        return df.mean(axis = 0)\n",
        "\n",
        "# flatten a list of lists\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "# Simple random sampling function.\n",
        "\n",
        "def simpleRandomSampling(prob):\n",
        "    a = random.randint(1, 100)\n",
        "    if a < prob:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def calculateChi(observed, expected):\n",
        "  b = expected\n",
        "  a = np.square(np.subtract(observed, expected))\n",
        "  return np.sum(np.sqrt(np.divide(a, b, out=np.zeros_like(a), where=b!=0)))\n",
        "\n",
        "def calculateOS(observed, expected):\n",
        "  b = expected\n",
        "  a = np.subtract(observed, expected)\n",
        "  return np.sum(np.abs(np.divide(a, b, out=np.zeros_like(a), where=b!=0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7JeKXanfQC3"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJvu0gJEfQC4"
      },
      "outputs": [],
      "source": [
        "# Reading and adjusting the data through header renaming and feature selection\n",
        "\n",
        "data = pd.read_csv(\"KDDTrain.txt\", sep =',', header = None)\n",
        "data.columns = [\"feature {}\".format(i+1) for i in range (43)]\n",
        "data = data.drop(data.columns[[0, 1, 2, 3, 8, 9, 11, 12, 14, 15, 16, 17, 18,\n",
        "                               19, 20, 21, 24, 25, 26, 28, 30, 31, 32, 33,\n",
        "                               34, 36, 37, 39, 40, 42]], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCpSUuVfr-Q"
      },
      "source": [
        "## Calculating Expected Standard Deviation, Mean and Median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "807JobiyfrDD",
        "outputId": "6086fb7c-6551-4d5a-bf5d-ef26ae7bd952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "472786.4310880747 10395.450230660043 54.0\n"
          ]
        }
      ],
      "source": [
        "expected_std = np.array(data.loc[:, data.columns != 'feature 42'].std())\n",
        "expected_mean = np.array(data.loc[:, data.columns != 'feature 42'].mean())\n",
        "expected_median = np.array(data.loc[:, data.columns != 'feature 42'].median()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrp_5NUDfQC4"
      },
      "source": [
        "## Data Normalization Using Min-Max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCzmeqTgfQC4"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "norm_array = scaler.fit_transform(data.loc[:, data.columns != 'feature 42'])\n",
        "norm_data = pd.DataFrame(norm_array, columns = data.columns[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Csp2QWfQC6"
      },
      "source": [
        "## Running the pseudo-code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAO5JdsvfQC6",
        "outputId": "80e47129-6470-40a0-c0d1-0056c0815194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clusters: \n",
            "1    1222\n",
            "2     424\n",
            "3     195\n",
            "4     194\n",
            "0     135\n",
            "5      84\n",
            "Name: cluster, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ],
      "source": [
        "# Taking N first rows of data for K-means algorithm.\n",
        "\n",
        "sampled_df=norm_data.head(N)\n",
        "kmeans = KMeans(n_clusters = n_clusters)\n",
        "kmeans.fit(sampled_df)\n",
        "labels = kmeans.predict(sampled_df)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "sampled_df['cluster'] = kmeans.labels_\n",
        "print('Clusters: ')\n",
        "print(sampled_df['cluster'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80xD8SFtfQC6",
        "outputId": "991bb7ad-fce4-4f40-a45c-ab3f5062e736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 0\n",
            "round 1\n",
            "round 2\n",
            "round 0\n",
            "round 0\n",
            "round 1\n",
            "round 2\n",
            "round 3\n",
            "round 4\n",
            "round 0\n",
            "round 1\n",
            "round 2\n",
            "round 3\n",
            "round 4\n",
            "round 5\n",
            "round 6\n",
            "round 7\n",
            "round 8\n",
            "round 9\n",
            "round 10\n",
            "round 11\n",
            "round 12\n",
            "round 13\n",
            "round 14\n",
            "round 15\n",
            "round 16\n",
            "round 17\n",
            "round 18\n",
            "round 19\n",
            "round 20\n",
            "round 0\n",
            "round 1\n",
            "round 2\n",
            "round 3\n",
            "round 0\n",
            "round 1\n",
            "round 2\n",
            "ERROR: Could not find file <ipython-input-13-29a5ff08db47>\n",
            "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n",
            "Entered !\n",
            "Cluster 1 size: 916 with 824 total changes. Entered at iteration 6200\n",
            "6200\n",
            "Program Executed in 9.541888153999992\n",
            "peak memory: 228.12 MiB, increment: 0.18 MiB\n"
          ]
        }
      ],
      "source": [
        "# Variables mapper:\n",
        "# initial_means => holds the mean value for each initial cluster obtained from learning base.\n",
        "# clusters => the initial clusters obtained from the learning base.\n",
        "# cluster_sample => the sampled clusters obtained.\n",
        "# Adding a timer for testing purposes\n",
        "\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Storing our cluster indices in the clusters lists, each list corresponding to a cluster index.\n",
        "\n",
        "clusters_size = sampled_df['cluster'].value_counts()\n",
        "clusters = init_list_of_objects(n_clusters)\n",
        "\n",
        "for i in range(N):\n",
        "    clusters[sampled_df['cluster'][i]].append(i)\n",
        "\n",
        "# Storing the initial mean values for all clusters.\n",
        "\n",
        "initial_means = []\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    initial_means.append(calculateMean(norm_data.iloc[clusters[i]]))\n",
        "\n",
        "# Initializing the sampled clusters.\n",
        "\n",
        "cluster_sample = init_list_of_objects(n_clusters)\n",
        "\n",
        "# Running our first block code of the algorithm.\n",
        "# While we haven't reached the sampling ratio desired, restart the loop.\n",
        "# Sample the data based on the probability given in the paper, combined with SRS.\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    j = 0\n",
        "    k = 0\n",
        "    while len(cluster_sample[i]) < int(sampling_ratio * len(clusters[i])):\n",
        "        if (clusters[i][j] not in cluster_sample[i]):\n",
        "            if k > 20:\n",
        "                cluster_sample[i].append(clusters[i][j])\n",
        "            if simpleRandomSampling(probability(norm_data.iloc[clusters[i][j]], centroids[i]) * (2**k)):\n",
        "                cluster_sample[i].append(clusters[i][j])\n",
        "        j = j + 1\n",
        "        if j == len(clusters[i]):\n",
        "            print(\"round \" + str(k))\n",
        "            k += 1\n",
        "            j = 0\n",
        "\n",
        "# Initialize the rank table.\n",
        "\n",
        "rank_table = init_list_of_dict(n_clusters)\n",
        "\n",
        "# Calculating the mean of each sample, as well as storing its size and the total changes.\n",
        "\n",
        "samples_means = []\n",
        "cluster_sizes = []\n",
        "total_changes = []\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    samples_means.append(calculateMean(norm_data.iloc[cluster_sample[i]]))\n",
        "    cluster_sizes.append(len(cluster_sample[i]))\n",
        "    total_changes.append(0)\n",
        "\n",
        "# Calculating the error to be stored in the rank table.\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    for j in range(len(cluster_sample[i])):\n",
        "        rank_table[i][cluster_sample[i][j]] = calculateErrorWithoutSample(norm_data.iloc[(cluster_sample[i][:j] + cluster_sample[i][j+1:])],\n",
        "                                                                          samples_means[i])\n",
        "\n",
        "# Sorting the rank table.\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    rank_table[i] = dict(\n",
        "        sorted(rank_table[i].items(), key=lambda item: item[1]))\n",
        "\n",
        "# Setting M to N because if we restart the cell execution, the last value of M will be stored.\n",
        "\n",
        "\n",
        "# For each remaining element, find its cluster and check if it can be sampled.\n",
        "@profile\n",
        "def algo():\n",
        "  M = N\n",
        "  while M < len(norm_data):\n",
        "      pred = kmeans.predict(norm_data.iloc[M: M+1])\n",
        "      error = calculateErrorWithSample(\n",
        "          norm_data.iloc[M: M+1], norm_data.iloc[cluster_sample[pred[0]]], initial_means[pred[0]])\n",
        "      error_out = calculateErrorWithoutSample(\n",
        "          norm_data.iloc[cluster_sample[pred[0]]], initial_means[pred[0]])\n",
        "      if error < error_out and error/error_out < 0.997:\n",
        "        if simpleRandomSampling(probability(norm_data.iloc[M], centroids[pred[0]])):\n",
        "            cluster_sample[pred[0]].append(M)\n",
        "            total_changes[pred[0]] = total_changes[pred[0]] + 1\n",
        "            cluster_sample[pred[0]].remove(list(rank_table[pred[0]].keys())[0])\n",
        "            del rank_table[pred[0]][list(rank_table[pred[0]].keys())[0]]\n",
        "\n",
        "      M = M + 1\n",
        "      start = timeit.default_timer()\n",
        "\n",
        "      if int(cluster_sizes[pred[0]] * change_ratio) == total_changes[pred[0]]:\n",
        "          print(\"Entered !\")\n",
        "          print(\"Cluster {0} size: {1} with {2} total changes. Entered at iteration {3}\".format(\n",
        "              pred[0], cluster_sizes[pred[0]], total_changes[pred[0]], M))\n",
        "          print(M)\n",
        "          total_changes[pred[0]] = 0\n",
        "          samples_means = []\n",
        "          for i in range(n_clusters):\n",
        "              samples_means.append(calculateMean(\n",
        "                  norm_data.iloc[cluster_sample[i]]))\n",
        "\n",
        "          for i in range(n_clusters):\n",
        "              for j in range(len(cluster_sample[i])):\n",
        "                  rank_table[i][cluster_sample[i][j]] = calculateErrorWithoutSample(norm_data.iloc[(cluster_sample[i][:j] + cluster_sample[i][j+1:])],\n",
        "                                                                                    samples_means[i])\n",
        "\n",
        "          for i in range(n_clusters):\n",
        "              rank_table[i] = dict(\n",
        "                  sorted(rank_table[i].items(), key=lambda item: item[1]))\n",
        "\n",
        "          stop = timeit.default_timer()\n",
        "          execution_time = stop - start\n",
        "\n",
        "          # Execution time in sec\n",
        "          print(\"Program Executed in \" + str(execution_time))\n",
        "\n",
        "algo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXNlkuBhkxtl"
      },
      "outputs": [],
      "source": [
        "final_cluster = data[data.index.isin(flatten(cluster_sample))]\n",
        "\n",
        "observed_mean =  np.array(final_cluster.\n",
        "                          loc[:, data.columns != 'feature 42'].mean())\n",
        "observed_median = np.array(final_cluster.\n",
        "                           loc[:, data.columns != 'feature 42'].median())\n",
        "observed_std = np.array(final_cluster.\n",
        "                        loc[:, data.columns != 'feature 42'].std())\n",
        "\n",
        "shi_square = []\n",
        "OS = []\n",
        "\n",
        "print(observed_mean)\n",
        "\n",
        "for i in range(len(observed_mean)):\n",
        "  shi_square.append(calculateChi(observed_mean[i], expected_mean[i]) \n",
        "                    + calculateChi(observed_std[i], expected_std[i]) \n",
        "                    + calculateChi(observed_median[i], expected_median[i]))\n",
        "  \n",
        "  OS.append(calculateOS(observed_mean[i], expected_mean[i]) \n",
        "            + calculateOS(observed_std[i], expected_std[i]) \n",
        "            + calculateOS(observed_median[i], expected_median[i]))\n",
        "  \n",
        "df = pd.DataFrame({'features': final_cluster.\n",
        "                   loc[:, data.columns != 'feature 42'].std().index,\n",
        "                   'mean':observed_mean,\n",
        "                   'median': observed_median,\n",
        "                   'standard deviation': observed_std,\n",
        "                   'OS': OS,\n",
        "                   'shi_square': shi_square})\n",
        "\n",
        "df.to_csv('file_name.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubN7R9lfQC7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "cluster_labels = []\n",
        "X = []\n",
        "\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(cluster_sample[i])):\n",
        "    X.append(norm_data.iloc[cluster_sample[i][j]].values)\n",
        "    cluster_labels.append(i)\n",
        "\n",
        "range_n_clusters = [n_clusters]\n",
        "\n",
        "for n in range_n_clusters:\n",
        "    fig, (ax1) = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(18, 7)\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    ax1.set_ylim([0, len(X) + (n + 1) * 10])\n",
        "    clusterer = KMeans(n_clusters=n, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"silhouette score \", silhouette_avg)\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        color = cm.nipy_spectral(float(i) / n)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "        y_lower = y_upper + 10\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Project.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "9b6f224b3d7daa80c0152910d5f33be0adb657cce3f5b079f96ab8316ce7174b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
